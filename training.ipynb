{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Increase RAM Reference Notes By Techhawa .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import load\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Build the model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "def build_siamese_model(inputShape, embeddingDim=48):\n",
        "    # Adapted from: https://www.pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/\n",
        "\n",
        "    # specify the inputs for the feature extractor network\n",
        "    inputs = Input(inputShape)\n",
        "    # define the first set of CONV => RELU => POOL => DROPOUT layers\n",
        "    x = Conv2D(64, (3, 4), padding=\"same\", activation=\"relu\")(inputs)\n",
        "    x = MaxPooling2D(pool_size=(3, 4))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    # second set of CONV => RELU => POOL => DROPOUT layers\n",
        "    x = Conv2D(64, (3, 4), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = MaxPooling2D(pool_size=3)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    # second set of CONV => RELU => POOL => DROPOUT layers\n",
        "    x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = MaxPooling2D(pool_size=2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    # prepare the final outputs\n",
        "    pooledOutput = GlobalAveragePooling2D()(x)\n",
        "\n",
        "\n",
        "    outputs = Dense(embeddingDim)(pooledOutput)\n",
        "    # build the model\n",
        "    model = Model(inputs, outputs)\n",
        "    # model.summary()\n",
        "    # return the model to the calling function\n",
        "    return model\n",
        "  \n",
        "def euclidean_distance(vectors):\n",
        "    # unpack the vectors into separate lists\n",
        "    (featsA, featsB) = vectors\n",
        "    # compute the sum of squared distances between the vectors\n",
        "    sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n",
        "    # return the euclidean distance between the vectors\n",
        "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "img_shape = (217, 334, 1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OsSdd3ox0msD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Load the batched dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmte7-M2HK-B"
      },
      "source": [
        "batch_x_train = np.load(\"/content/gdrive/MyDrive/Music Recommendation/datasets/32k/batch_x_train.npz\")\n",
        "batch_y_train = np.load(\"/content/gdrive/MyDrive/Music Recommendation/datasets/32k/batch_y_train.npz\")\n",
        "batch_x_test  = np.load(\"/content/gdrive/MyDrive/Music Recommendation/datasets/32k/batch_x_test.npz\")\n",
        "batch_y_test  = np.load(\"/content/gdrive/MyDrive/Music Recommendation/datasets/32k/batch_y_test.npz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Define the Pairs Generator Class to load the batched dataset pairs"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua7DxukxHFAF"
      },
      "source": [
        "class PairsGenerator(keras.utils.Sequence):\n",
        "    def __init__ (self, x_dataset, y_dataset):\n",
        "        self.x_dataset = x_dataset\n",
        "        self.y_dataset = y_dataset\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x_dataset)\n",
        "    \n",
        "    def __getitem__ (self, idx):\n",
        "        return [self.x_dataset['arr_'+str(idx)][:,0], self.x_dataset['arr_'+str(idx)][:,1]], self.y_dataset['arr_'+str(idx)]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Instansiate the batch generators"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc-ZpCRQHKcU"
      },
      "source": [
        "training_batch_generator = PairsGenerator(batch_x_train, batch_y_train)\n",
        "testing_batch_generator = PairsGenerator(batch_x_test, batch_y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Compile the model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbKZ9bD0HHnD"
      },
      "source": [
        "imgA = Input(shape=img_shape)\n",
        "imgB = Input(shape=img_shape)\n",
        "\n",
        "\n",
        "featureExtractor = build_siamese_model(img_shape)\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)\n",
        "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
        "\n",
        "model = Model(inputs=[imgA, imgB], outputs=outputs)\n",
        "\n",
        "epochs = 180  \n",
        "learning_rate = 0.0008\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(\n",
        "    learning_rate=learning_rate),\n",
        "    metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Start training"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "Cs1gUPjLz0fS",
        "outputId": "796eaf24-f68a-4dc7-adba-574fece53214"
      },
      "source": [
        "checkpoint_filepath = '/checkpoint_directory/'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "model.fit(\n",
        "    training_batch_generator,\n",
        "    validation_data=testing_batch_generator,\n",
        "    batch_size=32, \n",
        "    epochs=epochs,\n",
        "    callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}